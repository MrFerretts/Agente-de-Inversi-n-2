"""
LSTM DEEP LEARNING MODULE - Advanced Time Series Prediction
Sistema de Deep Learning con redes neuronales LSTM para predecir movimientos de precio
Captura patrones temporales que otros modelos no pueden detectar
"""

import pandas as pd
import numpy as np
from typing import Dict, Tuple, Optional, List
import pickle
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
    TENSORFLOW_AVAILABLE = True
except:
    TENSORFLOW_AVAILABLE = False
    print("‚ö†Ô∏è TensorFlow no disponible. Instala: pip install tensorflow")


class LSTMTradingModel:
    """
    Modelo LSTM (Long Short-Term Memory) para predecci√≥n de series temporales
    
    LSTM es ideal para trading porque:
    1. Tiene memoria de largo plazo (recuerda patrones de d√≠as/semanas atr√°s)
    2. Captura dependencias temporales complejas
    3. Maneja secuencias de longitud variable
    4. Detecta patrones no lineales
    """
    
    def __init__(self, 
                 prediction_days: int = 5,
                 lookback_window: int = 20,
                 threshold: float = 2.0):
        """
        Args:
            prediction_days: D√≠as hacia adelante para predecir
            lookback_window: Cu√°ntos d√≠as de historia usar (ventana temporal)
            threshold: % m√≠nimo de cambio para considerar "subida"
        """
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlow no est√° instalado. Ejecuta: pip install tensorflow")
        
        self.prediction_days = prediction_days
        self.lookback_window = lookback_window
        self.threshold = threshold
        
        self.model = None
        self.scaler = MinMaxScaler(feature_range=(0, 1))
        self.feature_names = []
        self.is_trained = False
        self.training_date = None
        self.model_metrics = {}
        self.training_history = None
        
    def create_sequences(self, data: np.ndarray, labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        Crea secuencias temporales para LSTM
        
        Ejemplo: Si lookback_window=20
        X[0] = [datos d√≠a 0-19] ‚Üí y[0] = sube/baja d√≠a 24
        X[1] = [datos d√≠a 1-20] ‚Üí y[1] = sube/baja d√≠a 25
        ...
        
        Args:
            data: Datos de features (escalados)
            labels: Etiquetas (0 o 1)
        
        Returns:
            X (secuencias), y (labels)
        """
        X, y = [], []
        
        for i in range(self.lookback_window, len(data) - self.prediction_days):
            # Secuencia de lookback_window d√≠as
            X.append(data[i-self.lookback_window:i])
            # Label es del d√≠a futuro
            y.append(labels[i + self.prediction_days - 1])
        
        return np.array(X), np.array(y)
    
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Prepara features espec√≠ficos para LSTM
        
        Args:
            df: DataFrame con indicadores b√°sicos
        
        Returns:
            DataFrame con features normalizados
        """
        data = df.copy()
        
        # ====================================================================
        # FEATURES PARA LSTM (optimizados para series temporales)
        # ====================================================================
        
        features = pd.DataFrame()
        
        # 1. Precio normalizado
        features['Close_Norm'] = data['Close'] / data['Close'].shift(20)
        
        # 2. Retornos
        for period in [1, 3, 5, 10]:
            features[f'Return_{period}'] = data['Close'].pct_change(period)
        
        # 3. Volatilidad rolling
        features['Volatility_10'] = data['Returns'].rolling(10).std()
        features['Volatility_20'] = data['Returns'].rolling(20).std()
        
        # 4. Momentum indicators
        features['RSI'] = data['RSI'] / 100  # Normalizar a [0, 1]
        features['StochRSI'] = data['StochRSI']
        
        # 5. Tendencia
        features['MACD_Hist_Norm'] = data['MACD_Hist'] / data['Close']
        features['ADX'] = data['ADX'] / 100
        
        # 6. Distancia a medias m√≥viles
        features['Dist_SMA20'] = (data['Close'] - data['SMA20']) / data['SMA20']
        features['Dist_SMA50'] = (data['Close'] - data['SMA50']) / data['SMA50']
        
        # 7. ATR normalizado
        features['ATR_Norm'] = data['ATR'] / data['Close']
        
        # 8. Volumen relativo
        features['RVOL'] = data['RVOL']
        
        # 9. Bollinger position
        bb_range = data['BB_Upper'] - data['BB_Lower']
        features['BB_Position'] = (data['Close'] - data['BB_Lower']) / (bb_range + 1e-9)
        
        # 10. Rangos de precio
        features['High_Low_Range'] = (data['High'] - data['Low']) / data['Close']
        
        # 11. Diferencias de precios (cambios)
        features['Close_Change'] = data['Close'].diff()
        features['Volume_Change'] = data['Volume'].diff()
        
        # 12. Medias m√≥viles de volumen
        features['Volume_MA_Ratio'] = data['Volume'] / data['Volume'].rolling(20).mean()
        
        return features.dropna()
    
    def build_model(self, input_shape: Tuple) -> Sequential:
        """
        Construye arquitectura LSTM
        
        Arquitectura:
        1. LSTM Layer 1 (128 units) - Captura patrones de largo plazo
        2. Dropout (0.2) - Previene overfitting
        3. LSTM Layer 2 (64 units) - Refina patrones
        4. Dropout (0.2)
        5. LSTM Layer 3 (32 units) - Extrae features finales
        6. Dropout (0.2)
        7. Dense (16 units) - Combinaci√≥n
        8. Output (1 unit, sigmoid) - Probabilidad
        
        Args:
            input_shape: (lookback_window, n_features)
        
        Returns:
            Modelo compilado
        """
        model = Sequential([
            # Primera capa LSTM - memoria de largo plazo
            LSTM(128, return_sequences=True, input_shape=input_shape),
            Dropout(0.2),
            BatchNormalization(),
            
            # Segunda capa LSTM - refina patrones
            LSTM(64, return_sequences=True),
            Dropout(0.2),
            BatchNormalization(),
            
            # Tercera capa LSTM - extrae features finales
            LSTM(32, return_sequences=False),
            Dropout(0.3),
            BatchNormalization(),
            
            # Capas densas para clasificaci√≥n
            Dense(16, activation='relu'),
            Dropout(0.2),
            
            # Output layer - probabilidad de subida
            Dense(1, activation='sigmoid')
        ])
        
        # Compilar con optimizer Adam
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
        )
        
        return model
    
    def train(self, data: pd.DataFrame, 
              epochs: int = 50,
              batch_size: int = 32,
              validation_split: float = 0.2) -> Dict:
        """
        Entrena el modelo LSTM
        
        Args:
            data: DataFrame con indicadores
            epochs: N√∫mero de √©pocas (ciclos de entrenamiento)
            batch_size: Tama√±o de batch
            validation_split: % de datos para validaci√≥n
        
        Returns:
            Dict con m√©tricas
        """
        print("\n" + "="*70)
        print("üß† ENTRENANDO MODELO LSTM (DEEP LEARNING)")
        print("="*70 + "\n")
        
        # ====================================================================
        # PREPARAR DATOS
        # ====================================================================
        
        print("üìä Preparando datos temporales...")
        
        # Crear features
        features_df = self.prepare_features(data)
        self.feature_names = list(features_df.columns)
        
        # Crear etiquetas
        df_with_labels = data.loc[features_df.index].copy()
        df_with_labels['Future_Return'] = df_with_labels['Close'].pct_change(self.prediction_days).shift(-self.prediction_days) * 100
        df_with_labels['Target'] = (df_with_labels['Future_Return'] > self.threshold).astype(int)
        
        # Alinear features y labels
        valid_idx = features_df.index.intersection(df_with_labels.dropna().index)
        features_array = features_df.loc[valid_idx].values
        labels_array = df_with_labels.loc[valid_idx, 'Target'].values
        
        if len(features_array) < self.lookback_window + self.prediction_days + 50:
            raise ValueError(f"Datos insuficientes. Necesitas al menos {self.lookback_window + self.prediction_days + 50} d√≠as.")
        
        print(f"‚úÖ Datos preparados: {len(features_array)} d√≠as")
        print(f"   - Features: {features_array.shape[1]}")
        print(f"   - Lookback window: {self.lookback_window} d√≠as")
        print(f"   - Clase 1 (subida): {labels_array.sum()} ({labels_array.sum()/len(labels_array)*100:.1f}%)")
        print(f"   - Clase 0 (no subida): {len(labels_array)-labels_array.sum()} ({(len(labels_array)-labels_array.sum())/len(labels_array)*100:.1f}%)")
        
        # ====================================================================
        # NORMALIZAR
        # ====================================================================
        
        print("\nüî¢ Normalizando datos...")
        features_scaled = self.scaler.fit_transform(features_array)
        
        # ====================================================================
        # CREAR SECUENCIAS
        # ====================================================================
        
        print(f"\n‚è±Ô∏è Creando secuencias temporales (ventana de {self.lookback_window} d√≠as)...")
        X, y = self.create_sequences(features_scaled, labels_array)
        
        print(f"‚úÖ Secuencias creadas: {X.shape[0]} secuencias")
        print(f"   Shape de X: {X.shape}")  # (n_samples, lookback_window, n_features)
        print(f"   Shape de y: {y.shape}")
        
        # ====================================================================
        # DIVIDIR TRAIN/TEST (temporal split, no shuffle)
        # ====================================================================
        
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        print(f"\nüìê Train/Test split:")
        print(f"   Train: {len(X_train)} secuencias")
        print(f"   Test:  {len(X_test)} secuencias")
        
        # ====================================================================
        # CONSTRUIR MODELO
        # ====================================================================
        
        print("\nüèóÔ∏è Construyendo arquitectura LSTM...")
        self.model = self.build_model(input_shape=(X_train.shape[1], X_train.shape[2]))
        
        print("\nüìã Arquitectura del modelo:")
        self.model.summary()
        
        # ====================================================================
        # CALLBACKS (para mejor entrenamiento)
        # ====================================================================
        
        callbacks = [
            # Early stopping - detiene si no mejora
            EarlyStopping(
                monitor='val_loss',
                patience=10,
                restore_best_weights=True,
                verbose=1
            ),
            # Reduce learning rate si se estanca
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=5,
                min_lr=0.00001,
                verbose=1
            )
        ]
        
        # ====================================================================
        # ENTRENAR
        # ====================================================================
        
        print(f"\nüöÄ Entrenando LSTM ({epochs} √©pocas)...")
        print("   (Esto puede tardar 2-5 minutos)\n")
        
        history = self.model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=validation_split,
            callbacks=callbacks,
            verbose=1
        )
        
        self.training_history = history.history
        
        print("\n‚úÖ Entrenamiento completado!")
        
        # ====================================================================
        # EVALUAR
        # ====================================================================
        
        print("\nüìä Evaluando modelo en test set...")
        
        # Predicciones
        y_pred_proba = self.model.predict(X_test, verbose=0).flatten()
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # M√©tricas
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred, zero_division=0)
        recall = recall_score(y_test, y_pred, zero_division=0)
        f1 = f1_score(y_test, y_pred, zero_division=0)
        
        try:
            auc_roc = roc_auc_score(y_test, y_pred_proba)
        except:
            auc_roc = 0.5
        
        # Guardar m√©tricas
        self.model_metrics = {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'auc_roc': auc_roc,
            'train_size': len(X_train),
            'test_size': len(X_test),
            'n_features': X_train.shape[2],
            'lookback_window': self.lookback_window,
            'epochs_trained': len(history.history['loss']),
            'final_train_loss': history.history['loss'][-1],
            'final_val_loss': history.history['val_loss'][-1],
            'features': self.feature_names
        }
        
        self.is_trained = True
        self.training_date = datetime.now()
        
        # ====================================================================
        # MOSTRAR RESULTADOS
        # ====================================================================
        
        print("\n" + "="*70)
        print("üèÜ RESULTADOS FINALES - LSTM")
        print("="*70)
        print(f"Accuracy:    {accuracy*100:>6.1f}%")
        print(f"Precision:   {precision*100:>6.1f}%")
        print(f"Recall:      {recall*100:>6.1f}%")
        print(f"F1-Score:    {f1*100:>6.1f}%")
        print(f"AUC-ROC:     {auc_roc:>6.3f}")
        print("="*70)
        
        print(f"\nüìâ Loss final:")
        print(f"   Train: {history.history['loss'][-1]:.4f}")
        print(f"   Val:   {history.history['val_loss'][-1]:.4f}")
        
        print(f"\n‚è±Ô∏è √âpocas entrenadas: {len(history.history['loss'])}/{epochs}")
        
        # An√°lisis de overfitting
        train_loss = history.history['loss'][-1]
        val_loss = history.history['val_loss'][-1]
        
        if val_loss > train_loss * 1.5:
            print("\n‚ö†Ô∏è Posible overfitting detectado (val_loss >> train_loss)")
            print("   Considera reducir epochs o agregar m√°s datos")
        elif abs(val_loss - train_loss) < 0.01:
            print("\n‚úÖ Buen balance entre train y validation")
        
        return self.model_metrics
    
    def predict(self, data: pd.DataFrame) -> Dict:
        """
        Predice con LSTM usando datos recientes
        
        Args:
            data: DataFrame con indicadores (debe tener al menos lookback_window d√≠as)
        
        Returns:
            Dict con predicci√≥n
        """
        if not self.is_trained:
            raise ValueError("Modelo no entrenado")
        
        # Preparar features
        features_df = self.prepare_features(data)
        
        if len(features_df) < self.lookback_window:
            raise ValueError(f"Necesitas al menos {self.lookback_window} d√≠as de datos")
        
        # Tomar √∫ltimos lookback_window d√≠as
        recent_data = features_df.iloc[-self.lookback_window:].values
        
        # Normalizar
        recent_scaled = self.scaler.transform(recent_data)
        
        # Reshape para LSTM: (1, lookback_window, n_features)
        X_pred = recent_scaled.reshape(1, self.lookback_window, -1)
        
        # Predecir
        prob_up = self.model.predict(X_pred, verbose=0)[0][0]
        prob_down = 1 - prob_up
        pred_class = int(prob_up > 0.5)
        
        # Calcular confianza
        confidence = max(prob_up, prob_down)
        
        if confidence > 0.75:
            confidence_level = "MUY ALTA"
        elif confidence > 0.65:
            confidence_level = "ALTA"
        elif confidence > 0.55:
            confidence_level = "MEDIA"
        else:
            confidence_level = "BAJA"
        
        # Recomendaci√≥n
        if prob_up > 0.75:
            recommendation = "COMPRA FUERTE"
        elif prob_up > 0.60:
            recommendation = "COMPRA"
        elif prob_up < 0.25:
            recommendation = "VENTA FUERTE"
        elif prob_up < 0.40:
            recommendation = "VENTA"
        else:
            recommendation = "MANTENER"
        
        return {
            'probability_up': float(prob_up),
            'probability_down': float(prob_down),
            'predicted_class': pred_class,
            'recommendation': recommendation,
            'confidence': float(confidence),
            'confidence_level': confidence_level,
            'prediction_days': self.prediction_days,
            'threshold': self.threshold,
            'model_accuracy': self.model_metrics['accuracy'],
            'model_type': 'LSTM',
            'lookback_window': self.lookback_window
        }
    
    def save_model(self, filepath: str):
        """Guarda modelo LSTM"""
        if not self.is_trained:
            raise ValueError("Modelo no entrenado")
        
        # Guardar modelo Keras
        model_path = filepath.replace('.pkl', '_keras.h5')
        self.model.save(model_path)
        
        # Guardar metadata
        metadata = {
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'model_metrics': self.model_metrics,
            'training_date': self.training_date,
            'prediction_days': self.prediction_days,
            'lookback_window': self.lookback_window,
            'threshold': self.threshold,
            'training_history': self.training_history
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(metadata, f)
        
        print(f"‚úÖ Modelo LSTM guardado:")
        print(f"   - Keras model: {model_path}")
        print(f"   - Metadata: {filepath}")
    
    def load_model(self, filepath: str):
        """Carga modelo LSTM"""
        if not os.path.exists(filepath):
            raise FileNotFoundError(f"Archivo no encontrado: {filepath}")
        
        # Cargar modelo Keras
        model_path = filepath.replace('.pkl', '_keras.h5')
        self.model = keras.models.load_model(model_path)
        
        # Cargar metadata
        with open(filepath, 'rb') as f:
            metadata = pickle.load(f)
        
        self.scaler = metadata['scaler']
        self.feature_names = metadata['feature_names']
        self.model_metrics = metadata['model_metrics']
        self.training_date = metadata['training_date']
        self.prediction_days = metadata['prediction_days']
        self.lookback_window = metadata['lookback_window']
        self.threshold = metadata['threshold']
        self.training_history = metadata['training_history']
        self.is_trained = True
        
        print(f"‚úÖ Modelo LSTM cargado: {filepath}")
        print(f"   Entrenado: {self.training_date.strftime('%Y-%m-%d %H:%M')}")
        print(f"   Accuracy: {self.model_metrics['accuracy']*100:.1f}%")


# ============================================================================
# FUNCI√ìN PARA STREAMLIT
# ============================================================================

def train_lstm_model(ticker: str, data_processed: pd.DataFrame,
                     prediction_days: int = 5,
                     lookback_window: int = 20,
                     epochs: int = 50) -> LSTMTradingModel:
    """
    Entrena modelo LSTM para un ticker
    
    Args:
        ticker: S√≠mbolo
        data_processed: DataFrame con indicadores
        prediction_days: D√≠as a predecir
        lookback_window: Ventana temporal
        epochs: √âpocas de entrenamiento
    
    Returns:
        Modelo entrenado
    """
    if not TENSORFLOW_AVAILABLE:
        print("‚ùå TensorFlow no disponible")
        print("   Instala con: pip install tensorflow")
        return None
    
    print(f"\n{'='*70}")
    print(f"üß† ENTRENANDO LSTM (DEEP LEARNING) PARA {ticker}")
    print(f"{'='*70}\n")
    
    model = LSTMTradingModel(
        prediction_days=prediction_days,
        lookback_window=lookback_window,
        threshold=2.0
    )
    
    try:
        metrics = model.train(data_processed, epochs=epochs, batch_size=32)
        return model
    except Exception as e:
        print(f"‚ùå Error: {str(e)}")
        import traceback
        traceback.print_exc()
        return None
